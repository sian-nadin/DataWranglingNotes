{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling: Gathering data\n",
    "Notes on methods for gathering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7dd3504c366f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to look at **Rotten Tomatoes top 100 movies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Rotten Tomatoes bestofrt TSV file into a DataFrame\n",
    "df = pd.read_csv('bestofrt.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if the file was imported correctly\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the movies on the list we can see that the critics scores and the audience scores don't line up. Some films, like ET, have quite a big difference in scores between critics and the audience. \n",
    "\n",
    "We can get the data on critics and audience scores using web scraping. We will use **Beautiful soup** to parse the HTML file on the top 100 movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing HTML file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for programmatically downloading HTML file to your computer:\n",
    "\n",
    "The two main ways to work with HTML files are:\n",
    "\n",
    "1. Saving the HTML file to your computer (using the Requests library for example) library and reading that file into a BeautifulSoup constructor\n",
    "2. Reading the HTML response content directly into a BeautifulSoup constructor (again using the Requests library for example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First method of working with HTML\n",
    "#This is just the code to download 1 file. We have 100 HTML files to access so we would have to put the code in a loop \n",
    "\n",
    "import requests\n",
    "url = \"https://www.rottentomatoes.com/m/t_the_extraterrestrial\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# #Save HTML to file\n",
    "# with open(\"et_the_extraterrestrial.html, mode='wb) as file:\n",
    "#      file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second method:\n",
    "# Work with HTML in memory\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Beautiful soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing you need to do is **make the soup**. That means passing the path to yout HTML file into a file handle, then passing that file handle into the Beautiful Soup constructor like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rt_html/et_the_extraterrestrial.html') as file:\n",
    "    soup = BeautifulSoup(file, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use  methods in the Beautiful soup library to easily find and extract data from this HTML. One of the most popular methods is the **find** method. Let's find the title of our movie using this method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll find that theres only one \" <title> tag in this whle HTML document and inside this tag is the title of the movie. \n",
    "soup.find('title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the title of the webpage and not the movie title only. To get just the movie title we'll have to do some string slicing. To access the contents of these tags we can use **.contents** which returns a list of the tags children. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('title').contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there's only one thing within this tag the list is one item long and we can therefore access it using the index 0. We can use this with string slicing to grab everything from the first charchter in our string to the 18th last character(The length of the string ' - Rotten Tomatoes'). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('title').contents[0][:-len(' - Rotten Tomatoes')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Jupyter Notebook below contains template code that:\n",
    "\n",
    "* Creates an empty list, df_list, to which dictionaries will be appended. This list of dictionaries will eventually be converted to a pandas DataFrame (this is the most efficient way of building a DataFrame row by row).\n",
    "* Loops through each movie's Rotten Tomatoes HTML file in the rt_html folder.\n",
    "* Opens each HTML file and passes it into a file handle called file.\n",
    "* Creates a DataFrame called df by converting df_list using the pd.DataFrame constructor.\n",
    "\n",
    "Your task is to extract the title, audience score, and number of audience ratings in each HTML file so each trio can be appended as a dictionary to df_list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# List of dictionaries to build file by file and later convert to a DataFrame\n",
    "df_list = []\n",
    "folder = 'rt_html'\n",
    "for movie_html in os.listdir(folder):\n",
    "    with open(os.path.join(folder, movie_html)) as file:\n",
    "        # Make the soup\n",
    "        soup = BeautifulSoup(file, \"html.parser\")\n",
    "        \n",
    "        #Find title. We don't wnat the '- Rotten Tomatoes' part of the title so only take the part before that\n",
    "        title = soup.find('title').contents[0][:-len(' - Rotten tomatoes')]\n",
    "        \n",
    "        #First find the div with class name audience-score meter then find the only span element within and grab the \n",
    "        # contents without the % sign\n",
    "        audience_score = soup.find('div', class_='audience-score meter').find('span').contents[0][:-1]\n",
    "        \n",
    "        #Find the div with number of ratings\n",
    "        num_audience_ratings = soup.find('div', class_ = 'audience-info hidden-xs superPageFontColor')\n",
    "        \n",
    "        #Find the second span tag within the div which contains number of reviews just want number of reviews and \n",
    "        # strip out whitespace. Will need to convert string to int so have to replace comma with empty\n",
    "        num_audience_ratings = num_audience_ratings.find_all('div')[1].contents[2].strip().replace(',', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roger Ebert review\n",
    "We are going to look at Rogert Eberts reviews for each of the movies listed in Rotten Tomatoes top 100 movies. The files for these reviews have been stored on a Udacity page. We will download them programatically using Pyhtons **Request** libraray.\n",
    "\n",
    "Python requests library has a method called **.get** which will send the request for us, return the contents of the file we requested, which we can then save to a file. \n",
    "\n",
    "Here's how it's done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "folder_name = 'ebert_review'\n",
    "if not os.path.exists(folder_name): #create folder if it doesn't exist already\n",
    "    os.makedirs(folder_name)\n",
    "    \n",
    "# Request code:\n",
    "url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9904_11-e.t.-the-extra-terrestrial/11-e.t.-the-extra-terrestrial.txt'\n",
    "response=requests.get(url)\n",
    "# We haven't actually saved the response to anything yet but let's take a look at what the response looks like\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response code 200 means everything went well with the request.\n",
    "\n",
    "All the text in our text file is in our computers working memory right now within this response variable. It's stored in the body of the response which we can access using **.content** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Response is in in Bytes format. \n",
    "* Using this and some basic file IO we're going to save this file to our computer. \n",
    "* We'll open a file called '11-e.t.-the-extra-terrestrial.txt', i.e. everything after the last / in the url. In order to get everything after the last / we'll use Python's split function. \n",
    "* We need to open the contents of this file which we'll then write the contents of the response variable to. We have to open this is wb mode (write binary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(folder_name,\n",
    "                      url.split('/')[-1]), mode='wb') as file:\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's how you download 1 file programatically. Let's check the contents of our folder ebert_reviews to make sure it worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have 100 files to download we'll use a for loop in conjunction with the provided ebert_review_urls list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebert_review_urls = ['https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9900_1-the-wizard-of-oz-1939-film/1-the-wizard-of-oz-1939-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9901_2-citizen-kane/2-citizen-kane.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9901_3-the-third-man/3-the-third-man.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9902_4-get-out-film/4-get-out-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9902_5-mad-max-fury-road/5-mad-max-fury-road.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9902_6-the-cabinet-of-dr.-caligari/6-the-cabinet-of-dr.-caligari.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9903_7-all-about-eve/7-all-about-eve.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9903_8-inside-out-2015-film/8-inside-out-2015-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9903_9-the-godfather/9-the-godfather.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9904_10-metropolis-1927-film/10-metropolis-1927-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9904_11-e.t.-the-extra-terrestrial/11-e.t.-the-extra-terrestrial.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9904_12-modern-times-film/12-modern-times-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9904_14-singin-in-the-rain/14-singin-in-the-rain.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9905_15-boyhood-film/15-boyhood-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9905_16-casablanca-film/16-casablanca-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9905_17-moonlight-2016-film/17-moonlight-2016-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9906_18-psycho-1960-film/18-psycho-1960-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9906_19-laura-1944-film/19-laura-1944-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9906_20-nosferatu/20-nosferatu.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9907_21-snow-white-and-the-seven-dwarfs-1937-film/21-snow-white-and-the-seven-dwarfs-1937-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9907_22-a-hard-day27s-night-film/22-a-hard-day27s-night-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9907_23-la-grande-illusion/23-la-grande-illusion.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9908_25-the-battle-of-algiers/25-the-battle-of-algiers.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9908_26-dunkirk-2017-film/26-dunkirk-2017-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9908_27-the-maltese-falcon-1941-film/27-the-maltese-falcon-1941-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9909_29-12-years-a-slave-film/29-12-years-a-slave-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9909_30-gravity-2013-film/30-gravity-2013-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9909_31-sunset-boulevard-film/31-sunset-boulevard-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad990a_32-king-kong-1933-film/32-king-kong-1933-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad990a_33-spotlight-film/33-spotlight-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad990a_34-the-adventures-of-robin-hood/34-the-adventures-of-robin-hood.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad990b_35-rashomon/35-rashomon.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad990b_36-rear-window/36-rear-window.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad990b_37-selma-film/37-selma-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad990c_38-taxi-driver/38-taxi-driver.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad990c_39-toy-story-3/39-toy-story-3.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad990c_40-argo-2012-film/40-argo-2012-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad990d_41-toy-story-2/41-toy-story-2.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad990d_42-the-big-sick/42-the-big-sick.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad990d_43-bride-of-frankenstein/43-bride-of-frankenstein.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad990d_44-zootopia/44-zootopia.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad990e_45-m-1931-film/45-m-1931-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad990e_46-wonder-woman-2017-film/46-wonder-woman-2017-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad990e_48-alien-film/48-alien-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad990f_49-bicycle-thieves/49-bicycle-thieves.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad990f_50-seven-samurai/50-seven-samurai.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad990f_51-the-treasure-of-the-sierra-madre-film/51-the-treasure-of-the-sierra-madre-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9910_52-up-2009-film/52-up-2009-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9910_53-12-angry-men-1957-film/53-12-angry-men-1957-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9910_54-the-400-blows/54-the-400-blows.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9911_55-logan-film/55-logan-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9911_57-army-of-shadows/57-army-of-shadows.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9912_58-arrival-film/58-arrival-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9912_59-baby-driver/59-baby-driver.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9913_60-a-streetcar-named-desire-1951-film/60-a-streetcar-named-desire-1951-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9913_61-the-night-of-the-hunter-film/61-the-night-of-the-hunter-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9913_62-star-wars-the-force-awakens/62-star-wars-the-force-awakens.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9913_63-manchester-by-the-sea-film/63-manchester-by-the-sea-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9914_64-dr.-strangelove/64-dr.-strangelove.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9914_66-vertigo-film/66-vertigo-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9914_67-the-dark-knight-film/67-the-dark-knight-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9915_68-touch-of-evil/68-touch-of-evil.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9915_69-the-babadook/69-the-babadook.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9915_72-rosemary27s-baby-film/72-rosemary27s-baby-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9916_73-finding-nemo/73-finding-nemo.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9916_74-brooklyn-film/74-brooklyn-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9917_75-the-wrestler-2008-film/75-the-wrestler-2008-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9917_77-l.a.-confidential-film/77-l.a.-confidential-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9918_78-gone-with-the-wind-film/78-gone-with-the-wind-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9918_79-the-good-the-bad-and-the-ugly/79-the-good-the-bad-and-the-ugly.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9918_80-skyfall/80-skyfall.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9919_82-tokyo-story/82-tokyo-story.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9919_83-hell-or-high-water-film/83-hell-or-high-water-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9919_84-pinocchio-1940-film/84-pinocchio-1940-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad9919_85-the-jungle-book-2016-film/85-the-jungle-book-2016-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad991a_86-la-la-land-film/86-la-la-land-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad991b_87-star-trek-film/87-star-trek-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad991b_89-apocalypse-now/89-apocalypse-now.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad991c_90-on-the-waterfront/90-on-the-waterfront.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad991c_91-the-wages-of-fear/91-the-wages-of-fear.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad991c_92-the-last-picture-show/92-the-last-picture-show.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad991d_93-harry-potter-and-the-deathly-hallows-part-2/93-harry-potter-and-the-deathly-hallows-part-2.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad991d_94-the-grapes-of-wrath-film/94-the-grapes-of-wrath-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad991d_96-man-on-wire/96-man-on-wire.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad991e_97-jaws-film/97-jaws-film.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad991e_98-toy-story/98-toy-story.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad991e_99-the-godfather-part-ii/99-the-godfather-part-ii.txt',\n",
    "                     'https://d17h27t6h515a5.cloudfront.net/topher/2017/September/59ad991e_100-battleship-potemkin/100-battleship-potemkin.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our text is stored in the body of the response which we can get using .content\n",
    "# Text stored in bytes format so open in mode='wb' \n",
    "for url in ebert_review_urls:\n",
    "# Use GET to obtain data from URL\n",
    "    response = requests.get(url)\n",
    "    with open(os.path.join(folder_name,\n",
    "# For file name get everything after the last slash in the url\n",
    "# Use split fxn to get last item in the list returned\n",
    "                      url.split('/')[-1]), mode='wb') as file:\n",
    "# Write to file handle we've opened\n",
    "        file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to make sure the files were downloaded\n",
    "os.listdir(folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting text from Ebert review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to select the right encoding to display the character sets properly. \n",
    "\n",
    "### Unicode and Python\n",
    "In Python 3, there is:\n",
    "* one text type: str, which holds Unicode data and\n",
    "* two byte types: bytes and bytearray\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gathering data from text files in Python means opening and reading from files. If you're using Pandas then this also means storing the text data you just read in a Pandas dataframe. \n",
    "\n",
    "We have 88 review to open and read. We'll need a loop to iterate through all of the files in the folder to open and read each. There are 2 main ways of doing this:\n",
    "1. Using the os library\n",
    "2. Using a libray called **glob**\n",
    "\n",
    "We've been using os.listdir so far which is good if you're sure you wnt to open every file in the folder. Let's switch it up and use glob instead. The glob library allows for using glob patterns to specify sets of file names. These glob patterns use wildcard characters. \n",
    "\n",
    "* **glob.glob** returns a list of path names that match *pathname*, i.e. the string aprameter you pass in\n",
    "We can use this to find all files that end in .txt, which in our folder is all of them. This returns a list that we can then loop through.\n",
    "* Make sure to **include encoding when opeing a file**. In order to find out what encoding to use we can go toone of the review pages on Robert Eberts site and **inspect source.** \n",
    "* When we open the file we don't want all of the text which would be done using print(file.read()). We want the following as seperate pieces of data:\n",
    "    * the first line a.k.a. the movie title (to merge to the master dataset with)\n",
    "    * the second line a.k.a. the review URL (not necessary for the word cloud but nice to have)\n",
    "    * everything from the third line onwards a.k.a. the review text\n",
    "* Since text files are seperated by new line characters and the file object returned from *with open as file* is an iterator we can read the file line by line. If you just want to read one line you use **.readline()**\n",
    "* There's actually a bit of whitespace after the movie titele which is the newline charcater. We can get rid of that by slicing it off at the end of the title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "for ebert_review in glob.glob('ebert_review/*.txt'):\n",
    "    with open(ebert_review, encoding='utf-8') as file: \n",
    "        title = file.readline()[:-1]       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want the review URL and the view text. But first we want all of this **data in a Pandas dataframe** so we need to build one. The most computationally efficient way to do that is to **create an empty list** and populate that list one by one as we iterate through this loop. We'll **fill this list with dictionaries** and the list of dictionaries will be later converted to a pnadas dataframe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dictionaries to build file by file and later convert to a DataFrame\n",
    "df_list = []\n",
    "for ebert_review in glob.glob('ebert_reviews/*.txt'):\n",
    "    with open(ebert_review, encoding='utf-8') as file:\n",
    "        title = file.readline()[:-1]\n",
    "        review_url = file.readline()[:-1]\n",
    "        review_text = file.read()\n",
    "\n",
    "        # Append to list of dictionaries\n",
    "        df_list.append({'title': title,\n",
    "                        'review_url': review_url,\n",
    "                        'review_text': review_text})\n",
    "df = pd.DataFrame(df_list, columns = ['title', 'review_url', 'review_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source: APIs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to get each movies poster. Since each movie has it's poster on it's Wikipedia page we can use Wikipedia's API to get the images. We'll be using the **Mediawiki API**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could have used Rotten Tomatoes API and the **rtsimple** libraray to get the Audience and critic scores. here's how it would have worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note:need API key for code to run\n",
    "#import rtsimple as rt\n",
    "\n",
    "#rt.API_KEY = \"YOUR KEY HERE\"\n",
    "#create movie object with the movie ID for E.T.\n",
    "#movie = rt.Movies('10489')\n",
    "#Access movie ratings\n",
    "#movie.ratings['audience_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we can't use the Rotten Tomatoes API (requires proposal form) we're going to use MediaWiki along with **wptools** library.\n",
    "\n",
    "* To get a **page object**, the usage is as follows: *page = wptools.page('Mahatma_Gandhi')* where 'Mahatma_Gandhi' is the last bit of the Wikipedia URL for that page. This page object has methods that can get us various pieces of data about that Wikipedia page, including all of the images on the page. \n",
    "* To get all of the data: *page = wptools.page('Mahatma_Gandhi').get()*\n",
    "* page now has the all associated attributes (such as image, infobox, links etc.), which can be accessed using dot notation through .data. e.g. *page.data['image']* would return a list of data for six images on this specific Wikipedia page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll get the page object for the E.T. The Extra-Terrestial Wikipedia page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wptools\n",
    "\n",
    "page = wptools.page('E.T._the_Extra-Terrestrial').get()\n",
    "\n",
    "# Accessing the image attribute will return the images for this page\n",
    "page.data['image']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON Files in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most data from APIs come in JSON or XML format. We'll focus on JSON.\n",
    "\n",
    "Sometimes we are limited in what we can represent in tabular data. \n",
    "* We might have fields that have multiple entries, like producers for a movie. \n",
    "* Fields may have subfields, like release date having a date and location of release.\n",
    "JSON is great for representing and accessing complicated data heirachies, like our Wikipedia infobox.\n",
    "\n",
    "* **JSON objects** are a collection of key value pairs. Objects are surrounded by curly braces. In Python JSON objects are **interpreted as dictionaries** and you can access them in the same way. \n",
    "* **JSON arrays** are an ordered list of values denoted by square brackets. In Python, JSON arrays are **interpreted as lists** and are accessed in the same way. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the wptools page object for the E.T. The Extra-Terrestial Wikipedia page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wptools\n",
    "page = wptools.page('E.T._the_Extra-Terrestrial').get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access the first image in the images attribute, which is a JSON array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.data['image'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access the director key of the infobox attribute, which is a JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.data['infobox']['director']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APIs, Downloading Files Programmatically, and JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two key things to be aware of before you begin:\n",
    "1. Wikipedia Page Titles. To access Wikipedia page data via the MediaWiki API with wptools, you need each movie's Wikipedia page title, i.e., what comes after the last slash in en.wikipedia.org/wiki/ in the URL. For this lesson,  the titles for each of the movies in the Top 100 has been complied.\n",
    "2. Downloading Image Files. You can use regular file opening, reading, and writing techniques, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "r = requests.get(url)\n",
    "with open(folder_name + '/' + filename, 'wb') as f:\n",
    "        f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this technique can be error-prone. It will work most of the time, but sometimes the file you write to will be damaged. This type of error is why the requests library maintainers recommend using the PIL library (short for Pillow) and BytesIO from the io library for non-text requests, like images. They recommend that you access the response body as bytes, for non-text requests. For example, to create an image from binary data returned by a request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "r = requests.get(url)\n",
    "i = Image.open(BytesIO(r.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though you may still encounter a similar file error, this code above will at least warn us with an error message, at which point we can manually download the problematic images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's gather the last piece of data for the Roger Ebert review word clouds now: the movie poster image files. Let's also keep each image's URL to add to the master DataFrame later.\n",
    "\n",
    "Though we're going to use a loop to minimize repetition, here's how the major parts inside that loop will work, in order:\n",
    "\n",
    "1. We're going to query the MediaWiki API using wptools to get a movie poster URL via each page object's image attribute.\n",
    "2. Using that URL, we'll programmatically download that image into a folder called bestofrt_posters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below contains code that:\n",
    "\n",
    "* Contains title_list, which is a list of all of the Wikipedia page titles for each movie in the Rotten Tomatoes Top 100 Movies of All Time list. This list is in the same order as the Top 100.\n",
    "* Creates an empty list, df_list, to which dictionaries will be appended. This list of dictionaries will eventually be converted to a pandas DataFrame (this is the most efficient way of building a DataFrame row by row).\n",
    "* Creates an empty folder, bestofrt_posters, to store the downloaded movie poster image files.\n",
    "* Creates an empty dictionary, image_errors, to fill to keep track of movie poster image URLs that don't work.\n",
    "* Loops through the Wikipedia page titles in title_list and:\n",
    "    * Stores the ranking of that movie in the Top 100 list based on its position in title_list. Ranking is needed so we can join this DataFrame with the master DataFrame later. We can't join on title because the titles of the Rotten Tomatoes pages and the Wikipedia pages differ.\n",
    "    * Uses try and except blocks to attempt to query MediaWiki for a movie poster image URL and to attempt to download that image. If the attempt fails and an error is encountered, the offending movie is documented in image_errors.\n",
    "    * Appends a dictionary with ranking, title, and poster_url as the keys and the extracted values for each as the values to df_list.\n",
    "* Inspects the images that caused errors and downloads the correct image individually (either via another URL in the image attribute's list or a URL from Google Images)\n",
    "* Creates a DataFrame called df by converting df_list using the pd.DataFrame constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "title_list = [\n",
    " 'The_Wizard_of_Oz_(1939_film)',\n",
    " 'Citizen_Kane',\n",
    " 'The_Third_Man',\n",
    " 'Get_Out_(film)',\n",
    " 'Mad_Max:_Fury_Road',\n",
    " 'The_Cabinet_of_Dr._Caligari',\n",
    " 'All_About_Eve',\n",
    " 'Inside_Out_(2015_film)',\n",
    " 'The_Godfather',\n",
    " 'Metropolis_(1927_film)',\n",
    " 'E.T._the_Extra-Terrestrial',\n",
    " 'Modern_Times_(film)',\n",
    " 'It_Happened_One_Night',\n",
    " \"Singin'_in_the_Rain\",\n",
    " 'Boyhood_(film)',\n",
    " 'Casablanca_(film)',\n",
    " 'Moonlight_(2016_film)',\n",
    " 'Psycho_(1960_film)',\n",
    " 'Laura_(1944_film)',\n",
    " 'Nosferatu',\n",
    " 'Snow_White_and_the_Seven_Dwarfs_(1937_film)',\n",
    " \"A_Hard_Day%27s_Night_(film)\",\n",
    " 'La_Grande_Illusion',\n",
    " 'North_by_Northwest',\n",
    " 'The_Battle_of_Algiers',\n",
    " 'Dunkirk_(2017_film)',\n",
    " 'The_Maltese_Falcon_(1941_film)',\n",
    " 'Repulsion_(film)',\n",
    " '12_Years_a_Slave_(film)',\n",
    " 'Gravity_(2013_film)',\n",
    " 'Sunset_Boulevard_(film)',\n",
    " 'King_Kong_(1933_film)',\n",
    " 'Spotlight_(film)',\n",
    " 'The_Adventures_of_Robin_Hood',\n",
    " 'Rashomon',\n",
    " 'Rear_Window',\n",
    " 'Selma_(film)',\n",
    " 'Taxi_Driver',\n",
    " 'Toy_Story_3',\n",
    " 'Argo_(2012_film)',\n",
    " 'Toy_Story_2',\n",
    " 'The_Big_Sick',\n",
    " 'Bride_of_Frankenstein',\n",
    " 'Zootopia',\n",
    " 'M_(1931_film)',\n",
    " 'Wonder_Woman_(2017_film)',\n",
    " 'The_Philadelphia_Story_(film)',\n",
    " 'Alien_(film)',\n",
    " 'Bicycle_Thieves',\n",
    " 'Seven_Samurai',\n",
    " 'The_Treasure_of_the_Sierra_Madre_(film)',\n",
    " 'Up_(2009_film)',\n",
    " '12_Angry_Men_(1957_film)',\n",
    " 'The_400_Blows',\n",
    " 'Logan_(film)',\n",
    " 'All_Quiet_on_the_Western_Front_(1930_film)',\n",
    " 'Army_of_Shadows',\n",
    " 'Arrival_(film)',\n",
    " 'Baby_Driver',\n",
    " 'A_Streetcar_Named_Desire_(1951_film)',\n",
    " 'The_Night_of_the_Hunter_(film)',\n",
    " 'Star_Wars:_The_Force_Awakens',\n",
    " 'Manchester_by_the_Sea_(film)',\n",
    " 'Dr._Strangelove',\n",
    " 'Frankenstein_(1931_film)',\n",
    " 'Vertigo_(film)',\n",
    " 'The_Dark_Knight_(film)',\n",
    " 'Touch_of_Evil',\n",
    " 'The_Babadook',\n",
    " 'The_Conformist_(film)',\n",
    " 'Rebecca_(1940_film)',\n",
    " \"Rosemary%27s_Baby_(film)\",\n",
    " 'Finding_Nemo',\n",
    " 'Brooklyn_(film)',\n",
    " 'The_Wrestler_(2008_film)',\n",
    " 'The_39_Steps_(1935_film)',\n",
    " 'L.A._Confidential_(film)',\n",
    " 'Gone_with_the_Wind_(film)',\n",
    " 'The_Good,_the_Bad_and_the_Ugly',\n",
    " 'Skyfall',\n",
    " 'Rome,_Open_City',\n",
    " 'Tokyo_Story',\n",
    " 'Hell_or_High_Water_(film)',\n",
    " 'Pinocchio_(1940_film)',\n",
    " 'The_Jungle_Book_(2016_film)',\n",
    " 'La_La_Land_(film)',\n",
    " 'Star_Trek_(film)',\n",
    " 'High_Noon',\n",
    " 'Apocalypse_Now',\n",
    " 'On_the_Waterfront',\n",
    " 'The_Wages_of_Fear',\n",
    " 'The_Last_Picture_Show',\n",
    " 'Harry_Potter_and_the_Deathly_Hallows_–_Part_2',\n",
    " 'The_Grapes_of_Wrath_(film)',\n",
    " 'Roman_Holiday',\n",
    " 'Man_on_Wire',\n",
    " 'Jaws_(film)',\n",
    " 'Toy_Story',\n",
    " 'The_Godfather_Part_II',\n",
    " 'Battleship_Potemkin'\n",
    "]\n",
    "\n",
    "folder_name = 'bestofrt_posters'\n",
    "# Make directory if it doesn't already exist\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "    \n",
    "# List of dictionaries to build and convert to a DataFrame later\n",
    "df_list = []\n",
    "image_errors = {}\n",
    "for title in title_list:\n",
    "    try:\n",
    "        # This cell is slow so print ranking to gauge time remaining\n",
    "        ranking = title_list.index(title) + 1\n",
    "        print(ranking)\n",
    "        page = wptools.page(title, silent=True)\n",
    "        # Your code here \n",
    "        images = page.data['image']\n",
    "        # First image is usually the poster\n",
    "        first_image_url = page.data['image'][0]\n",
    "        r = requests.get(first_image_url)\n",
    "        # Download movie poster image\n",
    "        i = Image.open(BytesIO(r.content))\n",
    "        image_file_format = first_image_url.split('.')[-1]\n",
    "        i.save(folder_name + \"/\" + str(ranking) + \"_\" + title + '.' + image_file_format)\n",
    "        # Append to list of dictionaries\n",
    "        df_list.append({'ranking': int(ranking),\n",
    "                        'title': title,\n",
    "                        'poster_url': first_image_url})\n",
    "    \n",
    "    # Not best practice to catch all exceptions but fine for this short script\n",
    "    except Exception as e:\n",
    "        print(str(ranking) + \"_\" + title + \": \" + str(e))\n",
    "        image_errors[str(ranking) + \"_\" + title] = images\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One you have completed the above code requirements, read and run the three cells below and interpret their output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in image_errors.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect unidentifiable images and download them individually\n",
    "for rank_title, images in image_errors.items():\n",
    "    if rank_title == '22_A_Hard_Day%27s_Night_(film)':\n",
    "        url = 'https://upload.wikimedia.org/wikipedia/en/4/47/A_Hard_Days_night_movieposter.jpg'\n",
    "    if rank_title == '53_12_Angry_Men_(1957_film)':\n",
    "        url = 'https://upload.wikimedia.org/wikipedia/en/9/91/12_angry_men.jpg'\n",
    "    if rank_title == '72_Rosemary%27s_Baby_(film)':\n",
    "        url = 'https://upload.wikimedia.org/wikipedia/en/e/ef/Rosemarys_baby_poster.jpg'\n",
    "    if rank_title == '93_Harry_Potter_and_the_Deathly_Hallows_–_Part_2':\n",
    "        url = 'https://upload.wikimedia.org/wikipedia/en/d/df/Harry_Potter_and_the_Deathly_Hallows_%E2%80%93_Part_2.jpg'\n",
    "    title = rank_title[3:]\n",
    "    df_list.append({'ranking': int(title_list.index(title) + 1),\n",
    "                    'title': title,\n",
    "                    'poster_url': url})\n",
    "    r = requests.get(url)\n",
    "    # Download movie poster image\n",
    "    i = Image.open(BytesIO(r.content))\n",
    "    image_file_format = url.split('.')[-1]\n",
    "    i.save(folder_name + \"/\" + rank_title + '.' + image_file_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame from list of dictionaries\n",
    "df = pd.DataFrame(df_list, columns = ['ranking', 'title', 'poster_url'])\n",
    "df = df.sort_values('ranking').reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving to a csv file is best for a simple datatframe like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll take some data that's already been cleaned since our data here hasn't been cleaned yet\n",
    "df = pd.read_csv('gathered_assessed_cleaned.csv')\n",
    "\n",
    "# Save the master DataFrame to a file called 'bestofrt_master.csv'\n",
    "df.to_csv('bestofrt_master.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relational databases in Python\n",
    "\n",
    "We're going to:\n",
    "\n",
    "1. Connect to a database. We'll connect to a SQLite database using SQLAlchemy, a database toolkit for Python.\n",
    "2. Store the data in the cleaned master dataset in that database. We'll do this using pandas' to_csv DataFrame method.\n",
    "3. Then read the brand new data in that database back into a pandas DataFrame. We'll do this using pandas' read_csv function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('bestofrt_master.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Connect to a database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create SQLAlchemy Engine and empty bestofrt database\n",
    "# bestofrt.db will not show up in the Jupyter Notebook dashboard yet\n",
    "engine = create_engine('sqlite:///bestofrt.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Store pandas DataFrame in database\n",
    "Store the data in the cleaned master dataset (bestofrt_master) in that database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store cleaned master DataFrame ('df') in a table called master in bestofrt.db\n",
    "# bestofrt.db will be visible now in the Jupyter Notebook dashboard\n",
    "df.to_sql('master', engine, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Read database data into a pandas DataFrame\n",
    "Read the brand new data in that database back into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gather = pd.read_sql('SELECT * FROM master', engine)\n",
    "df_gather.head(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
